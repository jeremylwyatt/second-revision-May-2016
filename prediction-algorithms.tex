% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

\usepackage{amsmath,amssymb,amsbsy}
\usepackage{algorithmic}

%%% The "real" document content comes below...

\title{Algorithms for Push Prediction}
\author{Jeremy L. Wyatt}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Definitions}

We first define an ordered set of rigid body transformations at time $t$ as $T^{z} = T^{z}(t)$, using the short-hand without the time index for compactness. $T^{z}_f$ denotes the subset of $T^{z}$ used for factor $f$ in KDEF. In general there is an invertible mapping between the ordered set $T^{z}_{f}(t)$ and a corresponding parameter vector $Z^t_{f}$ (or $Z_{f}$ for short) for a given parameterisation of rigid body transformations. It is these parameter vectors we use as kernel centres during learning and prediction.  We use $Z^{0:T}_{f}$ to denote a sequence of parameter vectors, and $\mathcal{Z} = \{Z\}$ to denote a set.

%\begin{equation}
%X \leftrightarrow T^{x} = \{ T^{A_{t}B_{t}}, T^{B_{t}O}, T^{A_{t}A_{t+1}}, T^{A^L_{t}B^L_{t}}, T^{A^L_{t}A^L_{t+1}}, \{ T^{E^{Sk}_{t}B^{Sk}_{t}} \}_{k = 1 \ldots N} \} 
%\end{equation}
%
%\begin{equation}
%Y \leftrightarrow T^{y} = \{ T^{B_{t}B_{t+1}}, T^{B^L_{t}B^L_{t+1}}, \{ T^{B^{Sk}_{t}B^{Sk}_{t+1}} \}_{k = 1 \ldots N} \}
%\end{equation}

\begin{equation}
X_g \leftrightarrow T^{x}_g = \{ T^{A_{t}B_{t}},  T^{B_{t}O} T^{A_{t}A_{t+1}} \} 
\end{equation}

\begin{equation}
Y_g \leftrightarrow T^{y}_g = \{ T^{B_{t}B_{t+1}} \}
\end{equation}

\begin{equation}
X_a \leftrightarrow T^{x}_a = \{ T^{A^L_{t}A^L_{t+1}}, T^{A^L_{t}B^L_{t}} \} 
\end{equation}

\begin{equation}
Y_a \leftrightarrow T^{y}_a  = \{ T^{B^L_{t}B^L_{t+1}} \}
\end{equation}

\begin{equation}
X_{e,k} \leftrightarrow T^{x}_{e,k} = \{ T^{E^{Sk}_{t}B^{Sk}_{t}} \} 
\end{equation}

\begin{equation}
Y_{e,k} \leftrightarrow T^{y}_{e,k} = \{  T^{B^{Sk}_{t}B^{Sk}_{t+1}}  \}
\end{equation}

\begin{equation}
X = (X_g,X_a,\{X_{e,k}\}_{k=1 \ldots N} )
\end{equation}

\begin{equation}
Y = (Y_g,Y_a,\{Y_{e,k}\}_{k=1 \ldots N} )
\end{equation}

In the current paper text, kernels are given as functions with arity three. Clearly that must mean the evaluation of a kernel for the value of the first argument. Instead here I use kernels $K(\mu,\sigma)$ of arity two, where $\mu$ is the kernel centre, and $\sigma$ is the kernel bandwidth. Also a set of kernels is denoted $\mathcal{K}$.

Given this we can define the learning algorithm given all the training trials for a single object for the KDEF algorithm, which is the most complex.

\begin{figure}
\caption{Learning with KDEF}
\label{alg:learning}
\begin{algorithmic}
\STATE {\bf learn-KDEF($X^{0:T}$,$Y^{0:T}$) $\rightarrow \mathcal{K}$}
\STATE $j=1$
\FOR{each of S trials}
\FOR{$t=0$ to $T-1$}
\STATE $K^{x,j}_{g}= K(X^t_{g}, H\bf{h^{(X)}})$
\STATE $K^{y,j}_{g}= K(Y^t_{g}, H\bf{h^{(Y)}})$
\STATE $K^{x,j}_{a}= K(X^t_{a}, H\bf{h^{(X)}})$
\STATE $K^{y,j}_{a}= K(Y^t_{a}, H\bf{h^{(Y)}})$
\FOR{$k=1$ to $N$}
\STATE $K^{x,j}_{e,k}= K(X^t_{e,k}, H\bf{h^{(X)}})$
\STATE $K^{y,j}_{e,k}= K(Y^t_{e,k}, H\bf{h^{(Y)}})$
\ENDFOR
\STATE $j=j+1$
\ENDFOR
\ENDFOR
\STATE $\mathcal{K} = \{ \{K^{x,j}_g ,  K^{y,j}_g \} , \{K^{x,j}_a ,  K^{y,j}_a \}, \{K^{x,j}_{e,k} ,  K^{y,j}_{e,k} \}_{k=1 \ldots N} \}_{ j = 1 \ldots M}$
\end{algorithmic}
\end{figure}

\begin{figure}
\caption{One-step prediction}
\label{alg:prediction}
\begin{algorithmic}
\STATE {\bf one-step-prediction-KDEF(X) $\rightarrow Y^*_g$}
\STATE $F = (g, a, (e,1) \ldots (e,N))$   
\FOR{$f \in F$}
\FOR{$j = 1$ to $M$}
\STATE $w_{f,j} = K^{x,j}_f(X_f)$
\ENDFOR
\STATE $\vec{w_f} =$ normalisation($w_{f,1} \ldots w_{f,M}$)
\ENDFOR

\FOR{$i=1$ to $\beta$}
\STATE randomly sample a factor $f\in F$ 
\STATE sample $j$ from distribution $\vec{w_f}$
\STATE  $\mathcal{Y}_i = Y_f$ from density $K^{y,j}_f$
\ENDFOR

\STATE maximise Eq.10 with $Y^*_g$ = stochastic-optimisation($\mathcal{Y}$,$\mathcal{K}$) 
\end{algorithmic}
\end{figure}

\end{document}
